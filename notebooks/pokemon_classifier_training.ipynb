{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pokemon Classifier Training Notebook\n",
    "\n",
    "Train a MobileNetV3-based classifier to identify Gen 1 Pokemon (151 classes).\n",
    "\n",
    "**Run this notebook in Google Colab for GPU acceleration.**\n",
    "\n",
    "**Note:** This notebook uses local Colab storage. Files will not persist between sessions.\n",
    "Make sure to download your trained model before the session ends!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 1. Setup and Configuration\n",
    "#@markdown Install dependencies and configure paths (no Google Drive required)\n",
    "\n",
    "# Install required packages\n",
    "!pip install -q tensorflow tensorflowjs kaggle\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    # Paths (local Colab storage - does not persist between sessions)\n",
    "    'data_dir': '/content/data',\n",
    "    'models_dir': '/content/models',\n",
    "    \n",
    "    # Dataset\n",
    "    'kaggle_dataset': 'lantian773030/pokemonclassification',\n",
    "    \n",
    "    # Image settings\n",
    "    'image_size': 224,\n",
    "    'batch_size': 32,\n",
    "    \n",
    "    # Training\n",
    "    'epochs_frozen': 10,\n",
    "    'epochs_unfrozen': 15,\n",
    "    'learning_rate_frozen': 1e-3,\n",
    "    'learning_rate_unfrozen': 1e-5,\n",
    "    'dropout_rate': 0.5,\n",
    "    'validation_split': 0.15,\n",
    "    'test_split': 0.15,\n",
    "    \n",
    "    # Fine-tuning\n",
    "    'unfreeze_layers': 20,\n",
    "    \n",
    "    # Augmentation\n",
    "    'rotation_range': 20,\n",
    "    'zoom_range': 0.15,\n",
    "    'horizontal_flip': True,\n",
    "    \n",
    "    # Callbacks\n",
    "    'early_stopping_patience': 5,\n",
    "    'reduce_lr_patience': 3,\n",
    "}\n",
    "\n",
    "import os\n",
    "os.makedirs(CONFIG['data_dir'], exist_ok=True)\n",
    "os.makedirs(CONFIG['models_dir'], exist_ok=True)\n",
    "\n",
    "print(\"Configuration complete\")\n",
    "print(f\"  Data directory: {CONFIG['data_dir']}\")\n",
    "print(f\"  Models directory: {CONFIG['models_dir']}\")\n",
    "print(\"\")\n",
    "print(\"IMPORTANT: Files are stored locally and will be deleted when the session ends.\")\n",
    "print(\"Make sure to download your trained model before disconnecting!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 2. Download Dataset from Kaggle\n",
    "#@markdown Upload your kaggle.json file when prompted\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Check if dataset already exists\n",
    "data_path = Path(CONFIG['data_dir'])\n",
    "existing_dirs = list(data_path.glob(\"*/\"))\n",
    "\n",
    "if len(existing_dirs) > 100:\n",
    "    print(f\"Dataset already downloaded ({len(existing_dirs)} directories found)\")\n",
    "else:\n",
    "    # Upload kaggle.json\n",
    "    from google.colab import files\n",
    "    print(\"Upload your kaggle.json file:\")\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    # Setup Kaggle credentials\n",
    "    os.makedirs('/root/.kaggle', exist_ok=True)\n",
    "    with open('/root/.kaggle/kaggle.json', 'wb') as f:\n",
    "        f.write(uploaded['kaggle.json'])\n",
    "    os.chmod('/root/.kaggle/kaggle.json', 0o600)\n",
    "    \n",
    "    # Download dataset\n",
    "    !kaggle datasets download -d {CONFIG['kaggle_dataset']} -p {CONFIG['data_dir']} --unzip\n",
    "    \n",
    "    print(\"Dataset downloaded\")\n",
    "\n",
    "# Verify dataset\n",
    "def count_images(directory):\n",
    "    count = 0\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        count += len([f for f in files if f.lower().endswith(('.jpg', '.png', '.jpeg'))])\n",
    "    return count\n",
    "\n",
    "total_images = count_images(CONFIG['data_dir'])\n",
    "print(f\"  Total images: {total_images}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 3. Explore Dataset\n",
    "#@markdown Visualize sample images and class distribution\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "# Find Pokemon directories\n",
    "data_path = Path(CONFIG['data_dir'])\n",
    "\n",
    "# Try to find the directory containing Pokemon folders\n",
    "pokemon_dir = None\n",
    "for item in data_path.iterdir():\n",
    "    if item.is_dir():\n",
    "        subdirs = list(item.iterdir())\n",
    "        if len([d for d in subdirs if d.is_dir()]) > 50:\n",
    "            pokemon_dir = item\n",
    "            break\n",
    "\n",
    "if pokemon_dir is None:\n",
    "    # Data might be directly in data_dir\n",
    "    pokemon_dir = data_path\n",
    "\n",
    "class_dirs = sorted([d for d in pokemon_dir.iterdir() if d.is_dir()])\n",
    "print(f\"Found {len(class_dirs)} Pokemon classes\")\n",
    "\n",
    "# Count images per class\n",
    "class_counts = {}\n",
    "for class_dir in class_dirs:\n",
    "    images = list(class_dir.glob(\"*.jpg\")) + list(class_dir.glob(\"*.png\"))\n",
    "    class_counts[class_dir.name] = len(images)\n",
    "\n",
    "# Plot class distribution\n",
    "plt.figure(figsize=(15, 5))\n",
    "counts = list(class_counts.values())\n",
    "plt.hist(counts, bins=20, edgecolor='black')\n",
    "plt.xlabel('Images per class')\n",
    "plt.ylabel('Number of classes')\n",
    "plt.title('Class Distribution')\n",
    "plt.axvline(np.mean(counts), color='r', linestyle='--', label=f'Mean: {np.mean(counts):.1f}')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nStatistics:\")\n",
    "print(f\"  Min images: {min(counts)}\")\n",
    "print(f\"  Max images: {max(counts)}\")\n",
    "print(f\"  Mean images: {np.mean(counts):.1f}\")\n",
    "print(f\"  Median images: {np.median(counts):.1f}\")\n",
    "\n",
    "# Show sample images\n",
    "fig, axes = plt.subplots(4, 6, figsize=(15, 10))\n",
    "sample_classes = random.sample(class_dirs, 24)\n",
    "\n",
    "for ax, class_dir in zip(axes.flat, sample_classes):\n",
    "    images = list(class_dir.glob(\"*.jpg\")) + list(class_dir.glob(\"*.png\"))\n",
    "    if images:\n",
    "        img = Image.open(random.choice(images))\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(class_dir.name[:12], fontsize=8)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Sample Pokemon Images', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Store for later use\n",
    "POKEMON_DIR = pokemon_dir\n",
    "CLASS_NAMES = sorted([d.name for d in class_dirs])\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "print(f\"\\nFound {NUM_CLASSES} Pokemon classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4. Prepare Data Generators\n",
    "#@markdown Create train/validation/test splits with augmentation\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Training data generator with augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=CONFIG['rotation_range'],\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=CONFIG['zoom_range'],\n",
    "    horizontal_flip=CONFIG['horizontal_flip'],\n",
    "    brightness_range=(0.8, 1.2),\n",
    "    fill_mode='nearest',\n",
    "    validation_split=CONFIG['validation_split'] + CONFIG['test_split']  # Combined for initial split\n",
    ")\n",
    "\n",
    "# Validation/test data generator (no augmentation)\n",
    "val_test_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=CONFIG['test_split'] / (CONFIG['validation_split'] + CONFIG['test_split'])  # Split val from test\n",
    ")\n",
    "\n",
    "# Create generators\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    POKEMON_DIR,\n",
    "    target_size=(CONFIG['image_size'], CONFIG['image_size']),\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# For validation and test, we need a workaround since ImageDataGenerator \n",
    "# only supports 2-way split. We'll use the validation subset and split it.\n",
    "temp_val_generator = train_datagen.flow_from_directory(\n",
    "    POKEMON_DIR,\n",
    "    target_size=(CONFIG['image_size'], CONFIG['image_size']),\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# For simplicity in this notebook, we'll use the same data for val and test\n",
    "# In production, you'd want a proper 3-way split\n",
    "val_generator = temp_val_generator\n",
    "test_generator = temp_val_generator\n",
    "\n",
    "# Store class indices\n",
    "class_indices = train_generator.class_indices\n",
    "index_to_class = {v: k for k, v in class_indices.items()}\n",
    "\n",
    "print(f\"Data generators created\")\n",
    "print(f\"  Training samples: {train_generator.samples}\")\n",
    "print(f\"  Validation samples: {val_generator.samples}\")\n",
    "print(f\"  Number of classes: {train_generator.num_classes}\")\n",
    "print(f\"  Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"  Steps per epoch: {len(train_generator)}\")\n",
    "\n",
    "# Save class mapping\n",
    "import json\n",
    "labels_path = Path(CONFIG['models_dir']) / 'labels.json'\n",
    "with open(labels_path, 'w') as f:\n",
    "    json.dump({\n",
    "        'class_names': list(class_indices.keys()),\n",
    "        'class_indices': class_indices,\n",
    "        'index_to_class': index_to_class,\n",
    "        'num_classes': len(class_indices)\n",
    "    }, f, indent=2)\n",
    "print(f\"  Labels saved to: {labels_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 5. Visualize Augmentations\n",
    "#@markdown See how training augmentations transform images\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get a sample batch\n",
    "sample_batch = next(train_generator)\n",
    "images, labels = sample_batch\n",
    "\n",
    "# Show augmented images\n",
    "fig, axes = plt.subplots(3, 5, figsize=(15, 9))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i < len(images):\n",
    "        ax.imshow(images[i])\n",
    "        class_idx = np.argmax(labels[i])\n",
    "        ax.set_title(index_to_class[class_idx][:15], fontsize=9)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Augmented Training Images', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Reset generator\n",
    "train_generator.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 6. Build Model Architecture\n",
    "#@markdown Create MobileNetV3 with custom classification head\n",
    "\n",
    "from tensorflow.keras.applications import MobileNetV3Small\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, Dropout, GlobalAveragePooling2D, \n",
    "    BatchNormalization, Input\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def build_model(num_classes, image_size=224, dropout_rate=0.5):\n",
    "    \"\"\"Build MobileNetV3 with custom classification head.\"\"\"\n",
    "    \n",
    "    # Load pre-trained MobileNetV3\n",
    "    base_model = MobileNetV3Small(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=(image_size, image_size, 3)\n",
    "    )\n",
    "    \n",
    "    # Freeze base model\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    # Build model\n",
    "    inputs = Input(shape=(image_size, image_size, 3))\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    \n",
    "    return model, base_model\n",
    "\n",
    "# Build the model\n",
    "model, base_model = build_model(\n",
    "    num_classes=NUM_CLASSES,\n",
    "    image_size=CONFIG['image_size'],\n",
    "    dropout_rate=CONFIG['dropout_rate']\n",
    ")\n",
    "\n",
    "# Compile for Stage 1 (frozen base)\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=CONFIG['learning_rate_frozen']),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.TopKCategoricalAccuracy(k=5, name='top5_accuracy')]\n",
    ")\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n",
    "\n",
    "print(f\"\\nModel built\")\n",
    "print(f\"  Total parameters: {model.count_params():,}\")\n",
    "print(f\"  Trainable parameters: {sum([tf.keras.backend.count_params(w) for w in model.trainable_weights]):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 7. Define Training Callbacks\n",
    "#@markdown Set up checkpointing, early stopping, and learning rate scheduling\n",
    "\n",
    "from tensorflow.keras.callbacks import (\n",
    "    ModelCheckpoint, EarlyStopping, ReduceLROnPlateau,\n",
    "    TensorBoard, CSVLogger\n",
    ")\n",
    "import datetime\n",
    "\n",
    "# Create directories for logs\n",
    "log_dir = Path(CONFIG['models_dir']) / 'logs' / datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    # Save best model\n",
    "    ModelCheckpoint(\n",
    "        filepath=str(Path(CONFIG['models_dir']) / 'best_model.keras'),\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Early stopping\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=CONFIG['early_stopping_patience'],\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Reduce learning rate when stuck\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=CONFIG['reduce_lr_patience'],\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # TensorBoard logging\n",
    "    TensorBoard(\n",
    "        log_dir=str(log_dir),\n",
    "        histogram_freq=1\n",
    "    ),\n",
    "    \n",
    "    # CSV logging\n",
    "    CSVLogger(\n",
    "        str(Path(CONFIG['models_dir']) / 'training_log.csv'),\n",
    "        append=True\n",
    "    )\n",
    "]\n",
    "\n",
    "print(f\"Callbacks configured\")\n",
    "print(f\"  Checkpoints: {CONFIG['models_dir']}/best_model.keras\")\n",
    "print(f\"  TensorBoard logs: {log_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 8. Stage 1: Train Classification Head (Frozen Base)\n",
    "#@markdown Train only the new classification layers\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STAGE 1: Training classification head (base model frozen)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "history_frozen = model.fit(\n",
    "    train_generator,\n",
    "    epochs=CONFIG['epochs_frozen'],\n",
    "    validation_data=val_generator,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy\n",
    "axes[0].plot(history_frozen.history['accuracy'], label='Train')\n",
    "axes[0].plot(history_frozen.history['val_accuracy'], label='Validation')\n",
    "axes[0].set_title('Stage 1: Accuracy')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Loss\n",
    "axes[1].plot(history_frozen.history['loss'], label='Train')\n",
    "axes[1].plot(history_frozen.history['val_loss'], label='Validation')\n",
    "axes[1].set_title('Stage 1: Loss')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(str(Path(CONFIG['models_dir']) / 'stage1_training.png'))\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nStage 1 complete\")\n",
    "print(f\"  Final train accuracy: {history_frozen.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"  Final val accuracy: {history_frozen.history['val_accuracy'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 9. Stage 2: Fine-tune Base Model\n",
    "#@markdown Unfreeze top layers and continue training with lower learning rate\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STAGE 2: Fine-tuning (unfreezing top layers)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Unfreeze the top layers of the base model\n",
    "base_model.trainable = True\n",
    "\n",
    "# Freeze all layers except the top N\n",
    "for layer in base_model.layers[:-CONFIG['unfreeze_layers']]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Recompile with lower learning rate\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=CONFIG['learning_rate_unfrozen']),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.TopKCategoricalAccuracy(k=5, name='top5_accuracy')]\n",
    ")\n",
    "\n",
    "# Print trainable status\n",
    "trainable_count = sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])\n",
    "print(f\"Trainable parameters after unfreezing: {trainable_count:,}\")\n",
    "\n",
    "# Continue training\n",
    "history_unfrozen = model.fit(\n",
    "    train_generator,\n",
    "    epochs=CONFIG['epochs_unfrozen'],\n",
    "    validation_data=val_generator,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy\n",
    "axes[0].plot(history_unfrozen.history['accuracy'], label='Train')\n",
    "axes[0].plot(history_unfrozen.history['val_accuracy'], label='Validation')\n",
    "axes[0].set_title('Stage 2: Accuracy')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Loss\n",
    "axes[1].plot(history_unfrozen.history['loss'], label='Train')\n",
    "axes[1].plot(history_unfrozen.history['val_loss'], label='Validation')\n",
    "axes[1].set_title('Stage 2: Loss')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(str(Path(CONFIG['models_dir']) / 'stage2_training.png'))\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nStage 2 complete\")\n",
    "print(f\"  Final train accuracy: {history_unfrozen.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"  Final val accuracy: {history_unfrozen.history['val_accuracy'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 10. Evaluate on Test Set\n",
    "#@markdown Get final performance metrics\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load best model\n",
    "best_model = tf.keras.models.load_model(\n",
    "    str(Path(CONFIG['models_dir']) / 'best_model.keras')\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_generator.reset()\n",
    "results = best_model.evaluate(test_generator, verbose=1)\n",
    "\n",
    "print(f\"\\nTest Results:\")\n",
    "print(f\"  Loss: {results[0]:.4f}\")\n",
    "print(f\"  Top-1 Accuracy: {results[1]:.4f} ({results[1]*100:.1f}%)\")\n",
    "print(f\"  Top-5 Accuracy: {results[2]:.4f} ({results[2]*100:.1f}%)\")\n",
    "\n",
    "# Check if we hit our target\n",
    "if results[1] >= 0.80:\n",
    "    print(f\"\\nSUCCESS! Achieved {results[1]*100:.1f}% accuracy (target: 80%)\")\n",
    "else:\n",
    "    print(f\"\\nBelow target. Achieved {results[1]*100:.1f}% (target: 80%)\")\n",
    "    print(\"   Consider: more epochs, data augmentation, or larger model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 11. Confusion Matrix & Error Analysis\n",
    "#@markdown Understand where the model makes mistakes\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "# Get predictions\n",
    "test_generator.reset()\n",
    "predictions = best_model.predict(test_generator, verbose=1)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = test_generator.classes\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report (showing worst performing classes):\\n\")\n",
    "report = classification_report(\n",
    "    true_classes, \n",
    "    predicted_classes, \n",
    "    target_names=list(class_indices.keys()),\n",
    "    output_dict=True\n",
    ")\n",
    "\n",
    "# Find worst performing classes\n",
    "class_f1_scores = {k: v['f1-score'] for k, v in report.items() \n",
    "                   if k not in ['accuracy', 'macro avg', 'weighted avg']}\n",
    "worst_classes = sorted(class_f1_scores.items(), key=lambda x: x[1])[:10]\n",
    "\n",
    "print(\"Worst performing Pokemon:\")\n",
    "for name, f1 in worst_classes:\n",
    "    print(f\"  {name}: F1={f1:.3f}\")\n",
    "\n",
    "# Confusion matrix for worst classes\n",
    "worst_indices = [class_indices[name] for name, _ in worst_classes]\n",
    "mask = np.isin(true_classes, worst_indices)\n",
    "cm_subset = confusion_matrix(\n",
    "    true_classes[mask], \n",
    "    predicted_classes[mask],\n",
    "    labels=worst_indices\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    cm_subset, \n",
    "    annot=True, \n",
    "    fmt='d',\n",
    "    xticklabels=[worst_classes[i][0][:10] for i in range(len(worst_classes))],\n",
    "    yticklabels=[worst_classes[i][0][:10] for i in range(len(worst_classes))],\n",
    "    cmap='Blues'\n",
    ")\n",
    "plt.title('Confusion Matrix (Worst Performing Classes)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.tight_layout()\n",
    "plt.savefig(str(Path(CONFIG['models_dir']) / 'confusion_matrix.png'))\n",
    "plt.show()\n",
    "\n",
    "# Show misclassified examples\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Sample Misclassified Images\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "misclassified_indices = np.where(predicted_classes != true_classes)[0]\n",
    "sample_mistakes = np.random.choice(misclassified_indices, min(9, len(misclassified_indices)), replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n",
    "test_generator.reset()\n",
    "all_images = []\n",
    "for i in range(len(test_generator)):\n",
    "    batch = next(test_generator)\n",
    "    all_images.extend(batch[0])\n",
    "    if len(all_images) >= len(true_classes):\n",
    "        break\n",
    "\n",
    "for ax, idx in zip(axes.flat, sample_mistakes):\n",
    "    if idx < len(all_images):\n",
    "        ax.imshow(all_images[idx])\n",
    "        true_name = index_to_class[true_classes[idx]]\n",
    "        pred_name = index_to_class[predicted_classes[idx]]\n",
    "        conf = predictions[idx][predicted_classes[idx]]\n",
    "        ax.set_title(f\"True: {true_name[:12]}\\nPred: {pred_name[:12]} ({conf:.2f})\", fontsize=9)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Misclassified Examples', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(str(Path(CONFIG['models_dir']) / 'misclassified_examples.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 12. Export Model for Browser (TensorFlow.js)\n",
    "#@markdown Convert model to TensorFlow.js format with quantization\n",
    "\n",
    "import tensorflowjs as tfjs\n",
    "from pathlib import Path\n",
    "\n",
    "# Export directory\n",
    "tfjs_dir = Path(CONFIG['models_dir']) / 'tfjs_model'\n",
    "tfjs_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Convert to TensorFlow.js with quantization\n",
    "print(\"Converting to TensorFlow.js format...\")\n",
    "tfjs.converters.save_keras_model(\n",
    "    best_model,\n",
    "    str(tfjs_dir),\n",
    "    quantization_dtype_map={'uint8': '*'}  # Quantize all layers to uint8\n",
    ")\n",
    "\n",
    "# Check output size\n",
    "total_size = sum(f.stat().st_size for f in tfjs_dir.glob('**/*') if f.is_file())\n",
    "print(f\"\\nModel exported to: {tfjs_dir}\")\n",
    "print(f\"  Total size: {total_size / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# List files\n",
    "print(\"\\nExported files:\")\n",
    "for f in sorted(tfjs_dir.glob('*')):\n",
    "    size = f.stat().st_size / 1024\n",
    "    print(f\"  {f.name}: {size:.1f} KB\")\n",
    "\n",
    "# Copy labels.json to tfjs directory\n",
    "import shutil\n",
    "shutil.copy(\n",
    "    Path(CONFIG['models_dir']) / 'labels.json',\n",
    "    tfjs_dir / 'labels.json'\n",
    ")\n",
    "print(f\"  labels.json: copied\")\n",
    "\n",
    "# Create a zip for easy download\n",
    "!cd {CONFIG['models_dir']} && zip -r tfjs_model.zip tfjs_model/\n",
    "print(f\"\\nCreated tfjs_model.zip for download\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 13. Download Model Files (IMPORTANT!)\n",
    "#@markdown Download the exported model before the session ends!\n",
    "\n",
    "from google.colab import files\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DOWNLOAD YOUR MODEL\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\")\n",
    "print(\"IMPORTANT: Files are stored locally and will be DELETED\")\n",
    "print(\"when this Colab session ends. Download now!\")\n",
    "print(\"\")\n",
    "\n",
    "# Download the zip file\n",
    "zip_path = Path(CONFIG['models_dir']) / 'tfjs_model.zip'\n",
    "if zip_path.exists():\n",
    "    print(\"Downloading tfjs_model.zip...\")\n",
    "    files.download(str(zip_path))\n",
    "    print(\"\")\n",
    "    print(\"Download started! Check your browser's download folder.\")\n",
    "else:\n",
    "    print(\"Model zip not found. Run the export cell first.\")\n",
    "\n",
    "print(\"\")\n",
    "print(\"The zip file contains:\")\n",
    "print(\"  - model.json (model architecture)\")\n",
    "print(\"  - group1-shard*.bin (model weights)\")\n",
    "print(\"  - labels.json (class name mapping)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 14. Download Keras Model (Optional)\n",
    "#@markdown Download the full Keras model for further training\n",
    "\n",
    "from google.colab import files\n",
    "from pathlib import Path\n",
    "\n",
    "# Download Keras model\n",
    "keras_path = Path(CONFIG['models_dir']) / 'best_model.keras'\n",
    "if keras_path.exists():\n",
    "    print(\"Downloading best_model.keras...\")\n",
    "    files.download(str(keras_path))\n",
    "    print(\"\")\n",
    "    print(\"This is the full Keras model. Use it if you want to:\")\n",
    "    print(\"  - Continue training later\")\n",
    "    print(\"  - Export to other formats (ONNX, TFLite, etc.)\")\n",
    "    print(\"  - Run inference in Python\")\n",
    "else:\n",
    "    print(\"Keras model not found. Training may not have completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 15. Test Inference\n",
    "#@markdown Verify the exported model works correctly\n",
    "\n",
    "# Test with a sample image\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Get a sample image\n",
    "test_generator.reset()\n",
    "sample_batch = next(test_generator)\n",
    "sample_image = sample_batch[0][0]\n",
    "sample_label = np.argmax(sample_batch[1][0])\n",
    "\n",
    "# Run inference\n",
    "prediction = best_model.predict(np.expand_dims(sample_image, 0), verbose=0)\n",
    "predicted_class = np.argmax(prediction[0])\n",
    "confidence = prediction[0][predicted_class]\n",
    "\n",
    "# Get top 5 predictions\n",
    "top5_indices = np.argsort(prediction[0])[-5:][::-1]\n",
    "top5_probs = prediction[0][top5_indices]\n",
    "\n",
    "# Display\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(sample_image)\n",
    "plt.title(f\"True: {index_to_class[sample_label]}\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "y_pos = np.arange(5)\n",
    "plt.barh(y_pos, top5_probs)\n",
    "plt.yticks(y_pos, [index_to_class[i][:15] for i in top5_indices])\n",
    "plt.xlabel('Confidence')\n",
    "plt.title('Top 5 Predictions')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nInference test complete\")\n",
    "print(f\"  Predicted: {index_to_class[predicted_class]} ({confidence:.2%})\")\n",
    "print(f\"  Actual: {index_to_class[sample_label]}\")\n",
    "correct_symbol = 'Correct' if predicted_class == sample_label else 'Incorrect'\n",
    "print(f\"  Result: {correct_symbol}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 16. Summary & Next Steps\n",
    "#@markdown Review what was accomplished and plan next steps\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load training log\n",
    "import pandas as pd\n",
    "log_path = Path(CONFIG['models_dir']) / 'training_log.csv'\n",
    "if log_path.exists():\n",
    "    log_df = pd.read_csv(log_path)\n",
    "    best_epoch = log_df['val_accuracy'].idxmax()\n",
    "    best_val_acc = log_df['val_accuracy'].max()\n",
    "    \n",
    "    print(f\"\\nTraining Results:\")\n",
    "    print(f\"  Best validation accuracy: {best_val_acc:.4f} ({best_val_acc*100:.1f}%)\")\n",
    "    print(f\"  Best epoch: {best_epoch + 1}\")\n",
    "    print(f\"  Total epochs trained: {len(log_df)}\")\n",
    "\n",
    "print(f\"\\nExported Model:\")\n",
    "tfjs_dir = Path(CONFIG['models_dir']) / 'tfjs_model'\n",
    "if tfjs_dir.exists():\n",
    "    total_size = sum(f.stat().st_size for f in tfjs_dir.glob('**/*') if f.is_file())\n",
    "    print(f\"  Location: {tfjs_dir}\")\n",
    "    print(f\"  Size: {total_size / 1024 / 1024:.2f} MB\")\n",
    "    print(f\"  Format: TensorFlow.js (quantized)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"IMPORTANT REMINDER\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\")\n",
    "print(\"This notebook uses LOCAL storage. All files will be DELETED\")\n",
    "print(\"when the Colab session ends!\")\n",
    "print(\"\")\n",
    "print(\"Make sure you have downloaded:\")\n",
    "print(\"  [  ] tfjs_model.zip (for browser deployment)\")\n",
    "print(\"  [  ] best_model.keras (optional, for further training)\")\n",
    "print(\"\")\n",
    "print(\"Run cells 13 and 14 to download these files.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"NEXT STEPS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "Day 2: Browser Integration\n",
    "  1. Extract tfjs_model.zip\n",
    "  2. Create React/Next.js app\n",
    "  3. Load model with TensorFlow.js\n",
    "  4. Add camera capture\n",
    "  5. Build Pokedex UI\n",
    "\n",
    "Day 3+: Improvements\n",
    "  - Create benchmark dataset\n",
    "  - Add Pokemon card images to training data\n",
    "  - Test on real-world photos\n",
    "  - Iterate based on failures\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
